{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour,RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder, minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import layers, models,Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#####\n",
    "\n",
    "from hopefullnet_files.general_processor import Utils\n",
    "from hopefullnet_files.models import HopefullNet\n",
    "from custom_utils import CustomUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **!! the template for normalization and oversampling is not mine, and hopefullnet is an neural network for motor imagery EEGs classification described here https://github.com/Kubasinska/MI-EEG-1D-CNN/blob/master/docs/hopefullnet.png !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## extract data:\n",
    "data extraction variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMP_RATE = 250\n",
    "FRAME_WIDTH = 640#4*SAMP_RATE   # in number of samples\n",
    "FRAME_STEP = 10              # jumps between frames, in nb of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames,classes=[],[]\n",
    "\n",
    "# storing filenames in a list\n",
    "files = glob.glob(\"./dataset Nathan/*/*\") \n",
    "print(files[1].split(\"/\")[-2])\n",
    "# extract data for every CSV files:\n",
    "classes_names = {'main gauche':0,'main droite':1,'deux mains':2,'deux pieds':3}\n",
    "        \n",
    "for filename in files:\n",
    "    CustomUtils.get_data_from_csv(filename,frames,classes,['EEG 3', 'EEG 4'],SAMP_RATE,FRAME_WIDTH,FRAME_STEP,classes_names)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CustomUtils.viz(np.array(frames[-1]).T,-100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.array(frames),np.array(classes)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization + oversampling\n",
    "reshaping puis division en train + test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_one_hot  = Utils.to_one_hot(y, by_sub=False)\n",
    "\n",
    "reshaped_x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])\n",
    "\n",
    "#separate a test set\n",
    "x_train_raw, x_valid_test_raw, y_train_raw, y_valid_test_raw = train_test_split(reshaped_x,\n",
    "                                                                            y_one_hot,\n",
    "                                                                            stratify=y_one_hot,\n",
    "                                                                            test_size=0.20,\n",
    "                                                                            random_state=42) #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalization (scaling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale indipendently train/test\n",
    "x_train_scaled_raw = minmax_scale(x_train_raw, axis=1) #2 <- 1\n",
    "x_test_valid_scaled_raw = minmax_scale(x_valid_test_raw, axis=1)#3 <-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "division du jeu de test,\n",
    "oversampling (equilibrer le jeu de données, algo **SMOTE**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Validation/test\n",
    "x_valid_raw, x_test_raw, y_valid, y_test = train_test_split(x_test_valid_scaled_raw,\n",
    "                                                    y_valid_test_raw,\n",
    "                                                    stratify=y_valid_test_raw,\n",
    "                                                    test_size=0.50,\n",
    "                                                    random_state=42) #4 <- 2,3,1\n",
    "\n",
    "#apply smote to train data\n",
    "print('classes count')\n",
    "print ('before oversampling = {}'.format(y_train_raw.sum(axis=0)))\n",
    "\n",
    "# smote\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy='all')\n",
    "\n",
    "x_train_smote_raw, y_train = sm.fit_resample(x_train_scaled_raw, y_train_raw)\n",
    "print('classes count')\n",
    "print ('before oversampling = {}'.format(y_train_raw.sum(axis=0)))\n",
    "print ('after oversampling = {}'.format(y_train.sum(axis=0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remise en forme d'origine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_smote_raw.reshape(x_train_smote_raw.shape[0], int(x_train_smote_raw.shape[1]/2), 2).astype(np.float64)\n",
    "\n",
    "x_valid = x_valid_raw.reshape(x_valid_raw.shape[0], int(x_valid_raw.shape[1]/2),2).astype(np.float64)\n",
    "x_test = x_test_raw.reshape(x_test_raw.shape[0], int(x_test_raw.shape[1]/2),2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# transfer learning\n",
    "à partir du modèle entrainé ***hopefullnet***\n",
    "* chargement du modèle, de ses poids\n",
    "* remplacement de la dernière couche\n",
    "* essai bloquage des poids des premières couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = HopefullNet_res()\n",
    "model.build(input_shape=(None,640,2))\n",
    "model.load_weights('./modelcheckpts/')\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.out = layers.Dense(4, activation='softmax')\n",
    "#for layer in model.layers[:3]:\n",
    "#  layer.trainable = False\n",
    "#model.load_weights('./unicornsave_3/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# entrainement du modèle \n",
    "hopefull net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(), #from_logits=True => softmax embedded in the loss function.\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint( # set model saving checkpoints\n",
    "    \"./unicornsave\", # set path to save model weights\n",
    "    monitor='val_loss', # set monitor metrics\n",
    "    verbose=1, # set training verbosity\n",
    "    save_best_only=True, # set if want to save only best weights\n",
    "    save_weights_only=False, # set if you want to save only model weights\n",
    "    mode='auto', # set if save min or max in metrics\n",
    "    period=100 # interval between checkpoints\n",
    "    )\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # set monitor metrics\n",
    "    min_delta=0.0001, # set minimum metrics delta\n",
    "    patience=10, # number of epochs to stop training\n",
    "    restore_best_weights=True, # set if use best weights or last weights\n",
    "    )\n",
    "callbacksList = [checkpoint, earlystopping] # build callbacks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "K.set_value(model.optimizer.learning_rate, 0.0001)\n",
    "history = model.fit(x_train, y_train, epochs=1000, \n",
    "                    validation_data=(x_valid, y_valid),batch_size=10, callbacks=callbacksList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "courbes, losses and accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('unicornmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss, testAcc = model.evaluate(x_test, y_test)\n",
    "\n",
    "Y_test = np.argmax(y_test, axis=1) # Convert one-hot to index\n",
    "y_pred = np.argmax(model.predict(x_test),axis=1)\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(Y_test, y_pred));  #annot=True to annotate cells, ftm='g' to disable scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(model)\n",
    "# model = tf.keras.models.load_model(\"./unicornmodel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's try with other recordings ( records that were not mixed in the train test splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_t,classes_t=[],[]\n",
    "\n",
    "# storing filenames in a list\n",
    "files = glob.glob(\"/test_data/*/*\") \n",
    "\n",
    "# extract data for every CSV files:\n",
    "for filename in files:\n",
    "    CustomUtils.get_data_from_csv(filename,frames_t,classes_t,['EEG 3', 'EEG 4'],SAMP_RATE,FRAME_WIDTH,FRAME_STEP)\n",
    "\n",
    "xt,yt = np.array(frames_t),np.array(classes_t)\n",
    "\n",
    "yt_one_hot  = Utils.to_one_hot(yt, by_sub=False)\n",
    "#Reshape for scaling\n",
    "reshaped_xt = xt.reshape(xt.shape[0], xt.shape[1] * xt.shape[2])\n",
    "xt_raw = minmax_scale(reshaped_xt, axis=1)#3 <-1\n",
    "xt_final = xt_raw.reshape(xt_raw.shape[0], int(xt_raw.shape[1]/2), 2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval, confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testLoss, testAcc = model.evaluate(xt_final,yt_one_hot)\n",
    "\n",
    "Y_test = np.argmax(yt_one_hot, axis=1) # Convert one-hot to index\n",
    "y_pred = np.argmax(model.predict(xt_final),axis=1)\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### do we have a chance to understand what is learnt?\n",
    "\n",
    "displaying feature importance... over 1d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,itsclass = np.array([x_test[10]]),np.array([y_test[0:10]])\n",
    "\n",
    "images = tf.Variable(img, dtype=float)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    pred = model(images, training=False)\n",
    "    class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]\n",
    "    loss = pred[0][class_idxs_sorted[0]]\n",
    "    \n",
    "grads = tape.gradient(loss, images)\n",
    "dgrad_abs = tf.math.abs(grads)\n",
    "dgrad_max_ = np.max(dgrad_abs, axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [i for i in range(len(img))]\n",
    "for channel in img:\n",
    "    plt.plot(channel*5)\n",
    "\n",
    "\n",
    "plt.plot((grad_eval)*2-5,alpha=0.8)\n",
    "plt.ylim(-10,10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
